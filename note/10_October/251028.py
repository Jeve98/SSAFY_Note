"""
[251028]

<AI 라이브 강의>

AI 가속기 : 정수 연산만 지원하거나 정수 연산에서 효율적으로 동작

양자화
- 일반적 양자화 : 무한한 실수를 유한한 관심 범위를 지정하여 표현, 이때 실수의 스케일링 파라미터 사용되므로 AI 가속기 구동이 불가
- 행렬곱 양자화 : 스케일링 파라미터를 정수화하여 정수 연산으로 유도
- 비트 쉬프팅 : 스케일링 파라미터를 뭉쳐서 M으로 나타냈으므로 이를 별도로 미리 양자화시켜 실수 연산을 제거

- 배치 정규화 : 학습 중 정규화를 통해 입력 분포의 변화를 줄여 안정적인 학습을 돕는 기법으로 실수 연산을 요구
>> 폴딩 기법 : 배치 정규화 연산을 바로 앞의 convolution 계층 연산에 통합해서 함께 양자화

- ViT 연산 양자화
    - Softmax : exponential 연산의 소수부를 정수로 근사하여 양자화
    - GELU : 상수(1.702)와 sigmoid 함수를 비트 쉬프팅, 정수 근사로 양자화
    - LayerNorm : 비트 쉬프팅 및 반복적 탐색을 통해 루트 연산 정수화


분포 이동 : 학습 데이터에서 경험하지 못했던 낯선 데이터를 마주하는 현상으로 주된 성능 하락의 원인
- TTA(Test Time Adaption) : 테스트 데이터의 분포를 파악하여 모델을 유연하게 적응시키는 방법
- TENT : 배치 정규화 계층을 테스트 데이터에 최적화시키는 방법으로 평균 분산을 업데이트 하거나 데스트 데이터에 대한 불확실성을 손실 함수로 하여 역전파를 통해 파라미터를 업데이트함
- SAR : 불확실성이 낮은 테스트 샘플을 선택해 모델 업데이트에 반영한 뒤 손실이 너무 요동치지 않도록 평탄화
- TPT(Test time Prompt Tuning) : 테스트 이미지마다 다수의 증강을 생성한 뒤 불확실성이 낮은 6개를 선택하여 이를 loss로 한 텍스트 프롬프트를 업데이트 하는 것으로 일관성 있는 예측을 보장
- PromptAlign : 학습 데이터 통계까지 활용해 분포 정렬을 수행하여 학습/테스트 테이터의 통계적 차이를 최소화하도록 설계하는 방법

※ 도메인 일반화 : 초거대 AI 학습 및 데이터 증강을 통해 분포 이동 현상을 회피하는 방법
    >> 스케일링 법칙에 기반하여 분포 이동의 리스크를 엄청난 규모로 회피
    ※ 스케일링 법칙 : 컴퓨팅 자원, 데이터가 많고 모델이 클수록 성능이 좋아지는 경향
    
    - 적응적 센싱 : 스케일링 법칙에 따라 모든 것을 준비하는 것은 불가능하고 physical ai에서는 이를 적용하기에 무리가 있으며 개인정보, 환경적 측면에서의 문제 발생을 해결할 목적으로 센서와 같은 감각기관을 고도화시켜 분포 이동을 억제하는 방법

    
<실습>

Multi-turn LLM : 기존의 대화 이력을 저장하며 이를 기반으로 맥락에 맞는 답변을 생성할 수 있으나 모든 이력이 더해지므로 장기 의존성 문제가 발생하고 사용 토큰이 기하급수적으로 증가함
>> 대화 이력의 요약을 이용


LangGraph : ReAct로 구성된 복잡한 AI Workflow 생성 툴
LangChain : 간단한 AI Workflow 혹은 Agentic AI 생성 툴로 확장
"""