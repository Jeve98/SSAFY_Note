"""
[251013]

<AI 라이브 강의>

AI(Artificial Intelligence) : 주어진 환경/데이터를 인지, 학습, 추론을 통해 목표 달성을 하도록 예측, 행동 선택, 계획하는 시스템
ML(Maching Learning) : AI 범주 내에서 데이터로부터 학습하여 목적을 달성하는 접근 방법론
DL(Deep Learning) : ML 범주 내에서 신경망 함수를 사용한 학습 방법론
    >> Data(Feature + Label)에서 규칙을 학습 == Data의 분포, 관계가 머신러닝의 학습 결과를 결정


학습 : 데이터와 성능척도를 바탕으로 가설공간의 후보 중 최적의 모델인 Feature - Label의 관계를 찾는 과정
    >> 예측, 중요 특성 파악, 해석 가능성 획득
    ※ Feature : 모델이 예측에 사용하는 입력정보로써 예측, 판단의 근거/단서가 됨
    ※ Label : 모델이 예측하려는 정답으로 학습의 목표값
가설 공간(Hypothesis Space) : 관계를 표현할 수 있는 모든 후보 함수들의 모음으로 Feature Space와 Label Space 위에서 정의된 함수들의 집합
모델 : 가설 공간에 속한 특정 함수


N-Dimension Feature Based Learning
    - 1 Dimension : Feature가 하나일 때 머신러닝이 학습하는 가장 단순한 형태
    - X Dimension
    
    ※ 측정 오차 Epsilon (!= 오차) : feature x와 독립적이며 Epsilon의 평균은 0으로 가정


지도 학습 : 처음 보는 데이터에서의 예측 성능 향상을 위해 feature와 label을 가지고 예측 규칙을 학습하는 방법
- 회귀 : 숫자인 label을 예측
    - MSE : 오차 제곱의 평균을 이용하여 큰 오류에 가중치를 둬, 전체 오류 수준을 확인할 수 있음
        ※ RMSE : 데이터와 같은 단위를 사용하기 위해 MSE에 제곱근을 취한 방법
        * 결정계수 (R^2) : label의 분산 중에서 feature로 설명되는 비율로 1이하의 값을 가지며 1에 가까울 수록 설명력이 높음

- 분류 : 범주인 label을 예측
    * Accuracy : 불균형 데이터에서 음성이 압도적으로 다수를 차지할 경우, 정확도가 높게 나오므로 다른 지표를 함께 확인해야 안전
    * 혼동행렬(Confusion Matrix) : 예측과 실제 값 사이의 관계를 행렬 형태로 표현 (TP, TN, FP_오탐, FN_누락)
        - 정밀도(Precision) : 양성이라 판정한 것 중, 진짜 양성 비율 (TP / (TP + FP))
        - 재현율(Recall) : 진짜 양성 가운데, 잡아낸 예측 양성 비율 (TP / (TP + FN))
        - F1-Score : 정밀도와 재현율의 조화 평균 (2 * (정밀도 * 재현율) / (정밀도 + 재현율))


Overfitting : 훈련 데이터의 우연한 패턴, 잡음까지 학습하여 테스트에서의 성능이 나빠지는 현상
    != 분포 변화(환경, 계절, 센서 변경 등)로 인한 에러 증가
       ex) 실사 학습, 카툰 테스트

Underfitting : 모델이 단순하거나 학습이 완료되지 않아, 오류가 큰 현상

훈련 오류 : 학습시킨 같은 데이터에 다시 적용해 계산한 오류
테스트 오류 : 새 관측치에 대해 모델을 적용했을 때의 평균 예측 오류
-> 모델 복잡성이 커질 수록 훈련 오류는 지속적으로 감소하지만 테스트 오류는 감소하다 다시 증가함 (Overfitting)
>> 데이터를 효율적으로 사용하여 일반화 오류를 해결
    - Hole Out : 셔플링한 데이터를 훈련셋과 검증셋으로 분할하여 각각 모델 적합, 검증 오류 계산 수행
        >> 어떤 표본이 훈련셋에 들어가느냐에 따라 검증 기반 테스트 오류 추정치가 매우 가변적이며 전체 데이터로 학습했을 때보다 성능이 낮게 추정(테스트 오류에 과대 추정)될 수 있음
    - K-fold Cross Validation : 셔플링한 데이터를 k개의 fold로 구분하여 각 fold에 대한 훈련, 검증을 수행한 이후 평균 오류로 테스트 오류를 추정
        ※ Leave-One-Out 교차검증 : n == k 인 경우(검증셋이 1인 경우)로 계산량이 매우 크지만 적절한 수준의 K-fold Cross Validation과 성능은 유사함


비지도 학습 :label 없이 데이터의 구조, 패턴, 집단을 찾아내는 학습 방법
- 클러스터링 : 데이터 안에서 하위 집단을 찾는 기법들의 총칭으로 내부적으로는 유사하되 집단 간에는 상이하도록 데이터를 분할하며 유사/상이 정도는 도메인 맥락에 따라 정의됨
    * k-means : 클러스터 수를 k개로 미리 정해 분할하되 클러스터 내부 변동의 합이 최소가 되도록 분할
        - 무작위로 클러스터 위치 초기회 이후 각 클러스터의 중심을 계산하여 각 관측치를 가장 가까운 중심의 클러스터에 재할당하는 것을 반복
    
    * 계층적 군집 : 클러스터 수를 정하지 않고 쌍별 비유사도를 계산, 유사한 것끼리 점차 병합한 뒤, 생성된 덴드로그램으로 기반으로 원하는 클러스터 수를 생성할 수 있는 높이에서 병합을 해제하여 분할(상향식)
        - Single Link : 두 클러스터 내 데이터 쌍별 거리 중 최소값을 군집 간 거리로 고려
        - Complete Link : 두 클러스터 내 데이터 쌍별 거리 중 최대값을 군집 간 거리로 고려
        - Average Link : 두 클러스터 내 데이터 쌍별 거리의 평균을 군집 간 거리로 고려
    
    >> 다수의 시도 권장
    
    ※ 스케일링 : 데이터를 표준화하여 표준 편차를 1로 변환하는 것이 요구됨


<실습>

머신러닝 : 데이터를 기반으로 최적의 모델을 계산하여 이를 기반으로 새로운 데이터를 예측/분류하는 방법
※ 모델 : 입력 데이터와 출력 데이터 간의 관계를 수식으로 표현한 함수
    - 학습 : 모델을 만드는 과정
    - 추론 : 모델을 기반으로 하는 예측

Feature
- 연속형 변수[number] : 수치가 연속적인 스펙트럼을 이루며 실수 형태로 측정되는 값
    >> 표준화, 정규화를 통해 전처리
- 범주형 변수[category, object] : 유한 개의 범주, 등급으로 구분되는 값으로 명목형, 순서형으로 나뉨
    >> 원-핫 인코딩, 순서형 인코딩, 임베딩을 통해 숫자화하여 전처리

데이터 정규화
- 표준 정규화(z-score 변환) : 평균 0, 분산 1로 각 feature를 변환하여 영향력을 통일하는 방법

- 브로드캐스팅 : 배열 연산 시 차원을 자동으로 확장하여 계산하는 방식


선형 회귀(Linear Regression) : feature의 추세(추세선/추세면)을 찾아내는 머신러닝 모델로 각 데이터의 오차 합을 최소로 하는 추세를 찾음
- Grid Search : 오차 합을 최소로 하는 추세를 찾기 위해 모든 데이터에 대한 검증을 진행하는 것    
    ※ '오차 합은 의미 없는 데이터이다' : 데이터 양에 따라 그 수치가 천차만별로 변화하기 대문
        >> 오차 평균의 경우, 예측 모델과 실제 값간의 차이를 설명하기에 보다 의미있는 데이터라고 할 수 있음
    
    - 손실함수 : 오차를 구하는 함수
        - MAE(Mean Absoulte Error) : 오차의 절댓값 평균 활용
        - MSE(Mean Squre Error) : 오차를 제곱하여 오차 값에 크게 반응하도록 MAE를 수정 (큰 오차에 큰 반응)

- 정규방정식(Normal Equation)
    : 오차 = y(실제값 벡터) - x(입력 feature vector) * Theta(가중치 vector)
      해당 행렬을 역행렬로 구하여 Theta를 도출
      -> Theta = (x^T x)^-1 x^T y
    
    ※ 역행렬 연산은 굉장히 느리고 불가능한 경우가 존재 >> SVD 기법(특이값 분해) 활용 - 메모리 사용량 계산이 난해

- 경사하강법(Gradient Descent) : 손실함수의 값(MSE)를 y축으로, 파라미터를 x축으로 하는 그래프에서 최소한의 데이터만을 활용하여 미분값이 0이 되는 지점을 찾음
    - 하이퍼 파라미터 : 학습되지 않는 값들로, 학습률, epoch(학습 횟수) 등이 존재하며 통상적으로 Grid Search를 통해 적절한 값을 탐색함

    ※ 지역 최소점(Local Minimum), 학습률에 따라 반복 지점(진동) 발생 가능

- 아담(Adam Optimizer) : 관성과 학습률 조율을 통해 지역 최소점 문제를 회피하는 경사하강법
    - 관성 : 이전의 방향과 동일한 방향으로 n번 추가 이동
    - 학습률 조율 : 반복 지점 발생 시, 학습률 변화


로지스틱 회귀(Logistic Regression) : 시그모이드 함수를 통해 두 범주 중 한 범주에 속할 확률을 예측하는 기법으로 분류 기법 중 하나
- 손실함수
    - Cross Entropy : 시그모이드 함수에서 MSE를 사용할 경우, 모든 오차가 작게 변화하므로(0 <= sigmoid <= 1) 작은 오차는 작게, 큰 오차는 크게 확대해주는 손실함수를 사용
"""