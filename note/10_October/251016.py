"""
[251016]

<AI 라이브 강의>

FCN : FCL(Fully-Connected Layer)를 적층하여 만든 모델
※ Fully-Connected Layer : 입력을 받아서 출력으로 변환하는 신경망의 기본 모듈로 입력 이미지를 1차원 벡터로 flattening 하여 선형 변환을 실시

CNN : 합성곱 레이어, 풀링 레이어를 적충하여 만든 모델로 계층별로 이미지의 지역적 템플릿을 확인
- 합성곱 레이어 (Convolution Layer) : 입력 이미지를 입력의 depth와 동일한 depth를 가진 filter와 연산하여 feature map을 뽑아내는 모듈로 3차원 구조를 보존하며 연산
    - 수용 영역 : CNN이 이미지를 처리하면서 한 번에 볼 수 있는 영역의 크기로 이미지 전체 맥락을 이해하는데 사용되며 네트워크가 깊어질 수록 함께 넓어짐
        -> 합성곱 레이어를 거치며 각 출력의 크기가 filter로 인해 점차 작은 크기로 압축

- 풀링 (Pooling) : 효율적 연산 및 위치 변화의 강건성 확보를 위해 추가한 레이어로 입력값을 풀링 레이어만큼 압축시킴
    ※ 위치 변화 강건성 : 입력 내 객체 위치가 다소 변해도 동일한 출력을 제공하는 것으로 저해상도 정보에 근거하여 작업을 수행함으로써 이미지의 화소 이동을 무시함
    - Max Pooling : 정해진 커널 사이즈로 이미지를 나누어 각 영역 내 가장 큰 값을 선택하는 연산

- 스트라이드 합성곱 : 필터를 스트라이드 값만큼 이동한 후 출력 연산하는 것으로 합성곱 레이어와 풀링 레이어를 하나의 레이어로 대체하여 이들을 함께 처리하는 효과를 내고 해상도 저하로 인한 정보 손실 역시도 줄어듬

>> 순서가 중요한 데이터 처리가 어렵고 긴 거리 의존성이 부족하며 픽셀 단위 복원이 불가 -> RNN, Attention, ViT(Vision Transformer) 사용


AlexNet : 5개의 합성곱 레이어, 3개의 FCL을 이용한 CNN 모델로 딥러닝을 이용한 첫 CNN 혁명

VGGNet : 공간 해상도를 줄이는 대신 깊이를 늘려 정보를 유지한다는 룰 아래에 만들어진 CNN 모델로 작고 단순한 필터를 깊게 쌓는 것으로 성능 향상을 꾀함
>> 단순한 설계로 모델 해석이 용이해졌고 특징 추출기, 전이학습에 강력한 베이스라인으로 작용함

ResNet : 합성곱(Convolution) 블록과 잔차(Residual) 블록을 활용한 CNN 모델로 잔차 블록을 통해 입력을 추가적으로 다음 블록으로 넘기는 연산을 진행하는 것으로 과거의 성능을 보장함

MobileNet : 공간과 채널을 2단계(깊이별/화소별)로 분리하여 처리하는 것으로 모바일/임베디드 환경에서 구동이 가능하도록 구성된 CNN 모델


RNN(Recurrent Neural Network) : sequential data 처리를 위해 고안된 신경망 구조

Cross-Attention : 둘 이상의 이종 데이터에서 Query/Key/Value를 정의하여 유사도를 반영하고 서로 다른 입력간 연결망을 구축

ViT 위치 인코딩


<실습>

토크나이제이션 : 문장을 입력값으로 사용하기 위해 토큰 단위로 잘라 수로 변환하는 과정
- 토큰화가 완성되어 있는 사전을 기반으로 입력 데이터를 사전에 있는 토큰으로 구성될 수 있도록 변환
    - Word Piece : 글자 단위로 분할한 후, 함께 등장하는 빈도가 높은 글자 조합의 점수를 측정하여 이를 기반으로 토큰을 선정
        ex) BERT : Word Piece 토크나이저를 사용한 Transformer 구조의 LLM 모델

    - Byte Level BPE : Byte 단위로 분할한 후, 함께 등장하는 빈도가 높은 Byte 조합으로 토큰을 선정

>> 단어의 접두사/접미사 등 변형을 반영하고 추론 속도 및 메모리 효율성을 증가시키며 Out of Vocabulary를 해결하기 위한 방법

※ 허깅페이스 : AI 툴, 모델을 오픈 소스로 공개하는 사이트


임베딩 모델 : 단어를 의미 공간이라는 벡터 공간에 맵핑하는 모델로 이를 통해 유사 단어를 알 수 있어, 신경망의 front에 별개로 배치됨
※ 임베딩 모델과 본 모델의 학습은 별개로 이루어짐

- Pooling : 다수의 벡터값을 하나로 합치는 것으로 이를 통해 각 단어에 대한 벡터를 모아 문장으로써 모델의 입력값으로 사용

- Cos 유사도 (L2 정규화) : 벡터의 방향을 기준으로 유사도를 판별하는데 이 때 두 벡터 사이의 방향 차이를 구하기 위한 방법
    - 크기 정보도 함께 사용하는 LLM에서는 잘 사용하지 않지만, 이미지/텍스트 검색과 같이 크기 정보가 필요 없는 경우, 주로 사용
"""