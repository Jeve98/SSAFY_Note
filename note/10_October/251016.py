"""
[251016]

<AI 라이브 강의>

FCN : FCL(Fully-Connected Layer)를 적층하여 만든 모델
※ Fully-Connected Layer : 입력을 받아서 출력으로 변환하는 신경망의 기본 모듈로 입력 이미지를 1차원 벡터로 flattening 하여 선형 변환을 실시

CNN : 합성곱 레이어, 풀링 레이어를 적충하여 만든 모델로 계층별로 이미지의 지역적 템플릿을 확인
- 합성곱 레이어 (Convolution Layer) : 입력 이미지를 입력의 depth와 동일한 depth를 가진 filter와 연산하여 feature map을 뽑아내는 모듈로 3차원 구조를 보존하며 연산
    - 수용 영역 : CNN이 이미지를 처리하면서 한 번에 볼 수 있는 영역의 크기로 이미지 전체 맥락을 이해하는데 사용되며 네트워크가 깊어질 수록 함께 넓어짐
        -> 합성곱 레이어를 거치며 각 출력의 크기가 filter로 인해 점차 작은 크기로 압축

- 풀링 (Pooling) : 효율적 연산 및 위치 변화의 강건성 확보를 위해 추가한 레이어로 입력값을 풀링 레이어만큼 압축시킴
    ※ 위치 변화 강건성 : 입력 내 객체 위치가 다소 변해도 동일한 출력을 제공하는 것으로 저해상도 정보에 근거하여 작업을 수행함으로써 이미지의 화소 이동을 무시함
    - Max Pooling : 정해진 커널 사이즈로 이미지를 나누어 각 영역 내 가장 큰 값을 선택하는 연산

- 스트라이드 합성곱 : 필터를 스트라이드 값만큼 이동한 후 출력 연산하는 것으로 합성곱 레이어와 풀링 레이어를 하나의 레이어로 대체하여 이들을 함께 처리하는 효과를 내고 해상도 저하로 인한 정보 손실 역시도 줄어듬

>> 순서가 중요한 데이터 처리가 어렵고 긴 거리 의존성이 부족하며 픽셀 단위 복원이 불가 -> RNN, Attention, ViT(Vision Transformer) 사용


AlexNet : 5개의 합성곱 레이어, 3개의 FCL을 이용한 CNN 모델로 딥러닝을 이용한 첫 CNN 혁명

VGGNet : 공간 해상도를 줄이는 대신 깊이를 늘려 정보를 유지한다는 룰 아래에 만들어진 CNN 모델로 작고 단순한 필터를 깊게 쌓는 것으로 성능 향상을 꾀함
>> 단순한 설계로 모델 해석이 용이해졌고 특징 추출기, 전이학습에 강력한 베이스라인으로 작용함

ResNet : 합성곱(Convolution) 블록과 잔차(Residual) 블록을 활용한 CNN 모델로 잔차 블록을 통해 입력을 추가적으로 다음 블록으로 넘기는 연산을 진행하는 것으로 과거의 성능을 보장함

MobileNet : 공간과 채널을 2단계(깊이별/화소별)로 분리하여 처리하는 것으로 모바일/임베디드 환경에서 구동이 가능하도록 구성된 CNN 모델


RNN(Recurrent Neural Network) : sequential data 처리를 위해 고안된 신경망 구조

Cross-Attention : 둘 이상의 이종 데이터에서 Query/Key/Value를 정의하여 유사도를 반영하고 서로 다른 입력간 연결망을 구축

ViT : 이미지 인식을 위해 고안된 Transformer 모델로 기존 Transformer의 인코더 부분만 사용하여 이미지를 패치로 분할한 뒤, 위치 정보를 추가하고 사용
- 증류학습 : 상당한 양의 데이터와 연산 자원을 대체하기 위한 학습 기법으로 Teacher-Student 모델을 기반으로 정답 라벨 뿐만 아니라 유사 클래스 간의 확률 분포를 함께 학습시키는 방법
※ Swin Transformer : 윈도우 영역을 지정하고 그 내부의 Attention에만 집중하는 ViT의 변종 모델


활성화 함수 : 입력 신호의 총합을 출력 신호로 변환하는 함수로 신경망에 비선형성을 부여하여 복잡한 패턴에 대한 학습을 실시함
- sigmoid : 입력이 매우 크거나 작으면 출력이 0 혹은 1에 고정되어 발생하는 기울기 소실 문제 및 출력 범위가 양수이기 때문에 편향된 업데이트가 발생
- tanh : 기울기 소실 문제
- ReLU : 영구적으로 죽은 뉴런이 발생할 수 있으며 0을 기준으로 비대칭이 발생
- Leaky ReLU : 음수 영역의 기울기 값에 임의성이 포함되고 0을 기준으로 한 비대칭은 여전
- ELU : 계산 복잡성 증가 및 추가적인 하이퍼 파라미터 요구


모델 정규화
- L2 정규화 : 큰 가중치에 제곱으로 패널티 제공
- L1 정규화 : 모든 가중치에 절댓값만큼 동일하게 패널티 제공 (중요하지 않은 가중치는 0으로 수렴)
- Elastic net : L1과 L2를 모두 사용한 방식으로 둘의 비중을 조절하며 진행하여 변수가 많고 상관관계가 높을 때 효과적 
- Dropout : 일부 뉴런을 의도적으로 끄는 방식으로 매 학습 스탭마다 다른 네트워크 구조가 샘플링되는 효과 발생


<실습>

토크나이제이션 : 문장을 입력값으로 사용하기 위해 토큰 단위로 잘라 수로 변환하는 과정
- 토큰화가 완성되어 있는 사전을 기반으로 입력 데이터를 사전에 있는 토큰으로 구성될 수 있도록 변환
    - Word Piece : 글자 단위로 분할한 후, 함께 등장하는 빈도가 높은 글자 조합의 점수를 측정하여 이를 기반으로 토큰을 선정
        ex) BERT : Word Piece 토크나이저를 사용한 Transformer 구조의 LLM 모델

    - Byte Level BPE : Byte 단위로 분할한 후, 함께 등장하는 빈도가 높은 Byte 조합으로 토큰을 선정

>> 단어의 접두사/접미사 등 변형을 반영하고 추론 속도 및 메모리 효율성을 증가시키며 Out of Vocabulary를 해결하기 위한 방법

※ 허깅페이스 : AI 툴, 모델을 오픈 소스로 공개하는 사이트


임베딩 모델 : 단어를 의미 공간이라는 벡터 공간에 맵핑하는 모델로 이를 통해 유사 단어를 알 수 있어, 신경망의 front에 별개로 배치됨
※ 임베딩 모델과 본 모델의 학습은 별개로 이루어짐

- Pooling : 다수의 벡터값을 하나로 합치는 것으로 이를 통해 각 단어에 대한 벡터를 모아 문장으로써 모델의 입력값으로 사용

- Cos 유사도 (L2 정규화) : 벡터의 방향을 기준으로 유사도를 판별하는데 이 때 두 벡터 사이의 방향 차이를 구하기 위한 방법
    - 크기 정보도 함께 사용하는 LLM에서는 잘 사용하지 않지만, 이미지/텍스트 검색과 같이 크기 정보가 필요 없는 경우, 주로 사용
"""